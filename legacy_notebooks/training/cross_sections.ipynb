{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85e14e0e",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-04-23T21:56:51.141209Z",
     "start_time": "2024-04-23T21:56:51.089041Z"
    }
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import pylab\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint\n",
    "from scipy.stats import chisquare, kstest\n",
    "from scipy.optimize import curve_fit\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import Callback, LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94775e58",
   "metadata": {},
   "source": [
    "# Reading model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee41d69b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T21:56:41.966146Z",
     "start_time": "2024-04-23T21:56:29.689355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:04, 55.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 271\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m\"\u001B[39m: optimizer,\n\u001B[1;32m    262\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr_scheduler\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    263\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscheduler\u001B[39m\u001B[38;5;124m\"\u001B[39m: lr_optim,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    267\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr_scheduler_monitoring\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m    268\u001B[0m                 }\n\u001B[1;32m    270\u001B[0m data \u001B[38;5;241m=\u001B[39m InterpolDataModule(hyperparams_dict)\n\u001B[0;32m--> 271\u001B[0m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m df \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdf\n\u001B[1;32m    273\u001B[0m df_all \u001B[38;5;241m=\u001B[39m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEbeam\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQ2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcos_theta\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphi\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "Cell \u001B[0;32mIn[54], line 122\u001B[0m, in \u001B[0;36mInterpolDataModule.setup\u001B[0;34m(self, stage)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm\u001B[38;5;241m.\u001B[39mtqdm(df\u001B[38;5;241m.\u001B[39mitertuples()):\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhyperparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maugment_factor\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m--> 122\u001B[0m         aug_series_list\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    124\u001B[0m aug_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(aug_series_list)\n\u001B[1;32m    125\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df, aug_df])\n",
      "Cell \u001B[0;32mIn[54], line 82\u001B[0m, in \u001B[0;36mInterpolDataModule.augment\u001B[0;34m(self, new_augm)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maugment\u001B[39m(\u001B[38;5;28mself\u001B[39m, new_augm):\n\u001B[0;32m---> 82\u001B[0m     augm \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mEbeam\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEbeam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEbeam\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mW\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mQ2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mQ2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mQ2\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcos_theta\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcos_theta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mabs\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcos_theta\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mphi\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphi\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdsigma_dOmega\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdsigma_dOmega\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43merror\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_augm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m                      \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m augm\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/pandas/core/series.py:536\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    534\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39m_mgr\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_dict_like(data):\n\u001B[0;32m--> 536\u001B[0m     data, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    537\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    538\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/pandas/core/series.py:650\u001B[0m, in \u001B[0;36mSeries._init_dict\u001B[0;34m(self, data, index, dtype)\u001B[0m\n\u001B[1;32m    647\u001B[0m     keys, values \u001B[38;5;241m=\u001B[39m default_index(\u001B[38;5;241m0\u001B[39m), []\n\u001B[1;32m    649\u001B[0m \u001B[38;5;66;03m# Input is now list-like, so rely on \"standard\" construction:\u001B[39;00m\n\u001B[0;32m--> 650\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;66;03m# Now we just make sure the order is respected, if any\u001B[39;00m\n\u001B[1;32m    653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/pandas/core/series.py:592\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    589\u001B[0m         data \u001B[38;5;241m=\u001B[39m SingleArrayManager\u001B[38;5;241m.\u001B[39mfrom_array(data, index)\n\u001B[1;32m    591\u001B[0m NDFrame\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data)\n\u001B[0;32m--> 592\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_axis(\u001B[38;5;241m0\u001B[39m, index)\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m original_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_pandas_object \u001B[38;5;129;01mand\u001B[39;00m data_dtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mobject_:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/pandas/core/generic.py:6314\u001B[0m, in \u001B[0;36mNDFrame.__setattr__\u001B[0;34m(self, name, value)\u001B[0m\n\u001B[1;32m   6311\u001B[0m \u001B[38;5;66;03m# if this fails, go on to more involved attribute setting\u001B[39;00m\n\u001B[1;32m   6312\u001B[0m \u001B[38;5;66;03m# (note that this matches __getattr__, above).\u001B[39;00m\n\u001B[1;32m   6313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set:\n\u001B[0;32m-> 6314\u001B[0m     \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__setattr__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6315\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata:\n\u001B[1;32m   6316\u001B[0m     \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, value)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/pandas/core/series.py:779\u001B[0m, in \u001B[0;36mSeries.name\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m    731\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    732\u001B[0m \u001B[38;5;124;03m    Return the name of the Series.\u001B[39;00m\n\u001B[1;32m    733\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;124;03m    'Even Numbers'\u001B[39;00m\n\u001B[1;32m    776\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name\n\u001B[0;32m--> 779\u001B[0m \u001B[38;5;129m@name\u001B[39m\u001B[38;5;241m.\u001B[39msetter\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mname\u001B[39m(\u001B[38;5;28mself\u001B[39m, value: Hashable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    781\u001B[0m     validate_all_hashable(value, error_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, value)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self, add_abc=False):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.add_abc = add_abc\n",
    "\n",
    "    @staticmethod\n",
    "    def func_cos(x, a, b, c):\n",
    "        return a + b*torch.cos(2*x) + c*torch.cos(x)\n",
    "\n",
    "    def forward(self, x, y_hat, y, w, A, B, C):\n",
    "        if self.add_abc:\n",
    "            phi = x[:, 4]\n",
    "            criterion = torch.sqrt(torch.mean(w*(y_hat - y)**2)/torch.sum(w)) + \\\n",
    "                        torch.mul(hyperparams_dict.get('abc_loss_factor'), torch.mean(torch.abs(w*y - self.func_cos(phi,A,B,C)))/torch.sum(w))\n",
    "        else:\n",
    "            criterion = torch.sqrt(torch.mean(w * (y_hat - y) ** 2) / torch.sum(w))\n",
    "        return criterion\n",
    "\n",
    "#params\n",
    "project_name = \"MSU_interpol\"\n",
    "\n",
    "logger_path = './wandb_local_logs'\n",
    "data_path = '../data/clasdb_pi_plus_n.txt'\n",
    "\n",
    "hyperparams_dict = {\n",
    "    'scale_data': False,\n",
    "    'augment': True,\n",
    "    'add_abc': False,\n",
    "    'abc_loss_factor': 1,\n",
    "    'augment_factor': 20,\n",
    "    'test_size': 0.1,\n",
    "    'batch_size': 256,\n",
    "    'net_architecture': [5,60,80,100,120,140,240,340,440,640,2000,1040,640,340,240,140,100,80,60,20,1],\n",
    "    'activation_function': nn.ReLU(),\n",
    "    'loss_func': 'RMSELoss()',\n",
    "    'optim_func': torch.optim.Adam,\n",
    "    'max_epochs': 2000,\n",
    "    'es_min_delta': 0.00001,\n",
    "    'es_patience': 50,\n",
    "    'lr': 0.001,\n",
    "    'lr_factor':0.5,\n",
    "    'lr_patience': 5,\n",
    "    'lr_cooldown': 20,\n",
    "}\n",
    "\n",
    "\n",
    "logger_full_path = os.path.join(logger_path, project_name, 'spring-feather-42')\n",
    "\n",
    "os.makedirs(logger_full_path, exist_ok=True)\n",
    "logging.basicConfig(encoding='utf-8',\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    handlers=[logging.FileHandler(os.path.join(logger_full_path, 'logs.log'), mode='w'),\n",
    "                              logging.StreamHandler(sys.stdout)],\n",
    "                    force=True)\n",
    "\n",
    "class InterpolDataSet(Dataset):\n",
    "    def __init__(self, features, labels, weights):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.len = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        weights = self.weights[index]\n",
    "        return feature, label, weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class InterpolDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super().__init__()\n",
    "        self.df = None\n",
    "        self.hyperparams = hyperparams\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def augment(self, new_augm):\n",
    "        augm = pd.Series({'Ebeam': np.random.normal(loc=new_augm.Ebeam, scale=new_augm.Ebeam/30),\n",
    "                           'W': np.random.normal(loc=new_augm.W, scale=new_augm.W/30),\n",
    "                           'Q2': np.random.normal(loc=new_augm.Q2, scale=new_augm.Q2/30),\n",
    "                           'cos_theta': np.clip(np.random.normal(loc=new_augm.cos_theta, scale=abs(new_augm.cos_theta/30)), -1, 1),\n",
    "                           'phi': np.clip(np.random.normal(loc=new_augm.phi, scale=new_augm.phi/30), 0, 2*np.pi),\n",
    "                           'dsigma_dOmega': np.random.normal(loc=new_augm.dsigma_dOmega, scale=new_augm.error/3),\n",
    "                           'error': new_augm.error,\n",
    "                           'weight': new_augm.weight,\n",
    "                          })\n",
    "        return augm\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # data reading and preprocessing\n",
    "        df = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "        df.columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'dsigma_dOmega', 'error', 'id']\n",
    "        df.loc[8314:65671, 'Ebeam'] = 5.754 # peculiarity of this dataset.\n",
    "        df['phi'] = df.phi.apply(lambda x: math.radians(x))\n",
    "        df['weight'] = df['error'].apply(lambda x: x and 1 / x or 100) # x and 1 / x or 100  is just a reversed error but with validation 1/0 error in this case it will return 100\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.iloc[df[['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']].drop_duplicates().index]\n",
    "\n",
    "        #train test split\n",
    "        feature_columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']\n",
    "        feature_columns_with_weights = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'weight']\n",
    "\n",
    "        feature_data = df[feature_columns_with_weights]\n",
    "        label_data = df['dsigma_dOmega']\n",
    "\n",
    "        if self.hyperparams.get('scale_data'):\n",
    "            scaler_feature = StandardScaler()\n",
    "            scaler_target = StandardScaler()\n",
    "            feature_data = scaler_feature.fit_transform(feature_data)\n",
    "            label_data = scaler_target.fit_transform(label_data.values.reshape(-1,1))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if self.hyperparams.get('augment'):\n",
    "            aug_series_list = []\n",
    "            for i in tqdm.tqdm(df.itertuples()):\n",
    "                for _ in range(self.hyperparams.get('augment_factor')):\n",
    "                    aug_series_list.append(self.augment(i))\n",
    "\n",
    "            aug_df = pd.DataFrame(aug_series_list)\n",
    "            df = pd.concat([df, aug_df])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        train_feature_data, val_feature_data, train_label_data, val_label_data = train_test_split(feature_data,\n",
    "                                                                                                  label_data,\n",
    "                                                                                                  test_size=self.hyperparams.get('test_size'),\n",
    "                                                                                                  random_state=1438)\n",
    "\n",
    "\n",
    "        self.train_dataset = InterpolDataSet(torch.tensor(train_feature_data[feature_columns].values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_label_data.values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['weight'].values, dtype=torch.float32))\n",
    "\n",
    "        self.val_dataset = InterpolDataSet(torch.tensor(val_feature_data[feature_columns].values, dtype=torch.float32),\n",
    "                                           torch.tensor(val_label_data.values, dtype=torch.float32),\n",
    "                                           torch.tensor(val_feature_data['weight'].values, dtype=torch.float32))\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.train_dataset, batch_size = self.hyperparams.get('batch_size'), shuffle = False, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.val_dataset, batch_size = self.hyperparams.get('batch_size'), shuffle = False, num_workers=0)\n",
    "\n",
    "class PrintCallbacks(Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        logging.info(\"Training is starting\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        logging.info(\"Training is ending\")\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\n",
    "        logging.info(f\"epoch: {pl_module.current_epoch}; train_loss: {epoch_mean}\")\n",
    "        pl_module.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch_mean = torch.stack(pl_module.validation_step_outputs).mean()\n",
    "        logging.info(f\"epoch: {pl_module.current_epoch}; val_loss: {epoch_mean}\")\n",
    "        pl_module.validation_step_outputs.clear()\n",
    "\n",
    "class InterpolRegressor(pl.LightningModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super(InterpolRegressor, self).__init__()\n",
    "\n",
    "        self.train_loss, self.train_mae, self.val_loss, self.val_mae = 0,0,0,0\n",
    "        self.hyperparams = hyperparams\n",
    "        self.save_hyperparameters(self.hyperparams)\n",
    "\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.loss_func = self.hyperparams.get('loss_func')\n",
    "\n",
    "        self.optim = self.hyperparams.get('optim_func')\n",
    "\n",
    "        self.net_architecture = self.hyperparams.get('net_architecture')\n",
    "        self.activation_function = self.hyperparams.get('activation_function')\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(1,len(self.net_architecture)):\n",
    "            self.net.append(nn.Linear(self.net_architecture[i-1], self.net_architecture[i]))\n",
    "            if i!=len(self.net_architecture)-1:\n",
    "                self.net.append(self.activation_function)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, w = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_func\n",
    "        self.train_loss = loss.forward(y_hat.reshape(-1), y, w)\n",
    "        self.train_mae = self.mae(y_hat.reshape(-1), y)\n",
    "\n",
    "        self.log('train_loss', self.train_loss, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('train_mae', self.train_mae, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "\n",
    "        self.training_step_outputs.append(self.train_loss)\n",
    "        return self.train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, w = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_func\n",
    "        self.val_loss = loss.forward(y_hat.reshape(-1), y, w)\n",
    "        self.val_mae = self.mae(y_hat.reshape(-1), y)\n",
    "\n",
    "        self.log('val_loss', self.val_loss, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('val_mae', self.val_mae, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "\n",
    "        self.validation_step_outputs.append(self.val_loss)\n",
    "        return self.val_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        sch = self.lr_schedulers()\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau) and self.trainer.current_epoch!=0:\n",
    "                sch.step(self.trainer.callback_metrics[\"val_loss\"])\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                            min_delta=self.hyperparams.get('es_min_delta'),\n",
    "                                            patience=self.hyperparams.get('es_patience'),\n",
    "                                            verbose=True)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(save_top_k=3,\n",
    "                                              monitor=\"val_loss\",\n",
    "                                              mode=\"min\",\n",
    "                                              dirpath=f\"{logger_full_path}/checkpoints\",\n",
    "                                              filename=\"{exp_name}{val_loss:.5f}-{epoch:02d}\")\n",
    "\n",
    "        lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "        print_callback = PrintCallbacks()\n",
    "\n",
    "        return [early_stop_callback, checkpoint_callback, print_callback, lr_monitor]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optim(self.parameters(), lr=self.hyperparams.get('lr'))\n",
    "        lr_optim = ReduceLROnPlateau(optimizer = optimizer,\n",
    "                                     mode = 'min',\n",
    "                                     factor = self.hyperparams.get('lr_factor'),\n",
    "                                     patience = self.hyperparams.get('lr_patience'),\n",
    "                                     cooldown=self.hyperparams.get('lr_cooldown'),\n",
    "                                     threshold=0.01,\n",
    "                                     verbose= True)\n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": lr_optim,\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"frequency\": 2,\n",
    "                    \"name\": 'lr_scheduler_monitoring'}\n",
    "                }\n",
    "\n",
    "data = InterpolDataModule(hyperparams_dict)\n",
    "data.setup('test')\n",
    "df = data.df\n",
    "df_all = df[['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']]\n",
    "df_target = df[['dsigma_dOmega']]\n",
    "\n",
    "model = InterpolRegressor.load_from_checkpoint(f'./wandb_local_logs/MSU_interpol_unified_notebooks/swift-shadow-13/checkpoints/exp_name=0val_loss=0.06049-epoch=208.ckpt', hyperparams=hyperparams_dict)\n",
    "\n",
    "model.eval()\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0014b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T19:19:34.043071Z",
     "start_time": "2024-03-07T19:19:34.020565Z"
    }
   },
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crossections"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T17:06:14.867711Z",
     "start_time": "2024-03-07T17:06:14.858012Z"
    }
   },
   "id": "9933eb299722c8a4"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ad29580",
   "metadata": {
    "code_folding": [
     0
    ],
    "ExecuteTime": {
     "end_time": "2024-04-23T21:47:53.895244Z",
     "start_time": "2024-04-23T21:47:53.880253Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_sections_check(df, E_beam, W, Q2, cos_theta):\n",
    "    df_example_set = df[(df.Ebeam == E_beam)&\n",
    "                        (df.W == W)&\n",
    "                        (df.Q2 == Q2)&\n",
    "                        (np.round(df.cos_theta, 1) == cos_theta)].sort_values('phi')\n",
    "    if len(df_example_set)==0:\n",
    "        print(len(df_example_set), E_beam, W, Q2, cos_theta)\n",
    "        return None\n",
    "\n",
    "    df_example_set_for_prediction = pd.DataFrame({'Ebeam' : [E_beam for _ in np.arange(0, 2*np.pi, 0.01)],\n",
    "                                                  'W' : [W for _ in np.arange(0, 2*np.pi, 0.01)],\n",
    "                                                  'Q2' : [Q2 for _ in np.arange(0, 2*np.pi, 0.01)],\n",
    "                                                  'cos_theta' : [cos_theta for _ in np.arange(0, 2*np.pi, 0.01)],\n",
    "                                                  'phi' : [phi for phi in np.arange(0, 2*np.pi, 0.01)]})\n",
    "\n",
    "#     return df_example_set\n",
    "    with torch.no_grad():\n",
    "        dsigma_dOmega_predicted = model.forward(torch.tensor(df_example_set_for_prediction.to_numpy(),dtype=torch.float32)).detach()\n",
    "        df_example_set_for_prediction['dsigma_dOmega_predicted'] = dsigma_dOmega_predicted\n",
    "\n",
    "    def func_cos(x, a, b, c): \n",
    "        return a + b*np.cos(2*x) + c*np.cos(x) \n",
    "\n",
    "    #input data \n",
    "    xdata = df_example_set.phi\n",
    "    ydata = df_example_set.dsigma_dOmega\n",
    "    ydata_error = df_example_set.error\n",
    "\n",
    "    #fitting the data \n",
    "    popt, pcov = curve_fit(func_cos, xdata, ydata, sigma=ydata_error, absolute_sigma=True)\n",
    "\n",
    "    a, b, c = popt[0], popt[1], popt[2]\n",
    "    #print the fitted parameters \n",
    "    print(\"a = %s , b = %s, c = %s\" % (a, b, c))\n",
    "    phi_theory = [i for i in np.arange(0, 2*np.pi, 0.01)]\n",
    "    dsigma_dOmega_theory = [func_cos(x, a, b, c) for x in phi_theory]\n",
    "\n",
    "    df_theory = pd.DataFrame({'phi_theory': phi_theory,\n",
    "                              'dsigma_dOmega_theory': dsigma_dOmega_theory})\n",
    "\n",
    "    df_chi_2 = pd.merge_asof(df_example_set, df_theory, left_on='phi', right_on='phi_theory')\n",
    "    df_chi_2 = pd.merge_asof(df_chi_2, df_example_set_for_prediction, on='phi')\n",
    "    df_chi_2 = df_chi_2[['phi', 'dsigma_dOmega', 'dsigma_dOmega_theory', 'dsigma_dOmega_predicted']]\n",
    "\n",
    "    real = df_chi_2['dsigma_dOmega'].apply(lambda x: np.round(x, 6)).values\n",
    "    theory = df_chi_2['dsigma_dOmega_theory'].apply(lambda x: np.round(x, 6)).values\n",
    "    preds = df_chi_2['dsigma_dOmega_predicted'].apply(lambda x: np.round(x, 6)).values\n",
    "\n",
    "    stat_theory_chi, p_value_theory_chi = np.round(chisquare(real, np.sum(real)/np.sum(theory)*theory), 3)\n",
    "    stat_preds_chi, p_value_preds_chi = np.round(chisquare(real, np.sum(real)/np.sum(preds)*preds), 3)\n",
    "\n",
    "    stat_theory_ks, p_value_theory_ks = np.round(kstest(real, theory), 2)\n",
    "    stat_preds_ks, p_value_preds_ks = np.round(kstest(real, preds), 2)\n",
    "\n",
    "    plt.figure(figsize=(22, 6), dpi=80)\n",
    "    plt.plot(phi_theory, \n",
    "             dsigma_dOmega_theory, \n",
    "             label=f'fitted, chi^2 = {stat_theory_chi}, p_value = {p_value_theory_chi}, ks = {stat_theory_ks}, p_value = {p_value_theory_ks}')\n",
    "    plt.plot(df_example_set_for_prediction.phi, \n",
    "             dsigma_dOmega_predicted, \n",
    "             color='green',\n",
    "             label=f'predicted; chi^2 = {stat_preds_chi}, p_value = {p_value_preds_chi}, ks = {stat_preds_ks}, p_value = {p_value_preds_ks}')\n",
    "    plt.scatter(x=df_example_set.phi, \n",
    "                y=df_example_set.dsigma_dOmega,\n",
    "                color='red', marker='*', label=f\"real_data with params Ebeam: {E_beam}, W: {W}, Q2: {Q2}, cos_theta: {cos_theta}\")\n",
    "    plt.errorbar(x=df_example_set.phi,\n",
    "                 y=df_example_set.dsigma_dOmega,\n",
    "                 yerr=df_example_set.error,\n",
    "                 color='red',\n",
    "                 fmt='o')\n",
    "\n",
    "    plt.xlabel(\"phi: rad\", fontsize=\"20\")\n",
    "    plt.ylabel(\"dsigma_dOmega: barn\", fontsize=\"20\")\n",
    "    \n",
    "    plt.legend(loc =\"upper right\", fontsize=\"15\")\n",
    "    plt.savefig(f'./checks_unified/3_models/phi_cross_sections/E_beam={E_beam}/Q2={Q2}/W={W}/E_beam={E_beam}_Q2={Q2}_W={W}_cos_theta={cos_theta}.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37b88315",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-23T21:49:38.872082Z",
     "start_time": "2024-04-23T21:49:38.860308Z"
    }
   },
   "outputs": [],
   "source": [
    "E_beam = 1.515\n",
    "\n",
    "for Q2 in tqdm.tqdm(np.sort(df[df.Ebeam == E_beam].Q2.unique())):\n",
    "    os.makedirs(f\"./checks_unified/3_models/phi_cross_sections/E_beam={E_beam}/Q2={Q2}\")\n",
    "    for W in tqdm.tqdm([1.23, 1.53, 1.715, 1.95]):\n",
    "        os.makedirs(f\"./checks_unified/3_models/phi_cross_sections/E_beam={E_beam}/Q2={Q2}/W={W}\")\n",
    "        for cos_theta in [-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "            try:\n",
    "                cross_sections_check(df, E_beam, W, Q2, cos_theta)\n",
    "            except:\n",
    "                print(E_beam, W, Q2, cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Structure functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb6db7c906f3708b"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f205d33c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-02-15T00:10:10.727463Z",
     "start_time": "2024-02-15T00:09:44.345085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:16<00:00, 13.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_grid\n",
    "Ebeam = 5.754\n",
    "step_W = 0.005\n",
    "step_Q2 = 0.1\n",
    "step_cos_theta = 0.1\n",
    "step_phi = 0.05\n",
    "\n",
    "\n",
    "data_grid = []\n",
    "for W in tqdm.tqdm(np.arange(1.1, 2.2 + step_W, step_W)):\n",
    "    for Q2 in np.arange(1.6, 4.3 + step_Q2, step_Q2):\n",
    "         for cos_theta in np.arange(-1, 1+step_cos_theta, step_cos_theta):\n",
    "                for phi in np.arange(0, 2*np.pi, step_phi):\n",
    "                    data_grid.append([Ebeam,\n",
    "                                      W,\n",
    "                                      Q2,\n",
    "                                      cos_theta,\n",
    "                                      phi])\n",
    "\n",
    "df_grid = pd.DataFrame(data_grid)\n",
    "df_grid.columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']\n",
    "\n",
    "\n",
    "df_grid.W = np.round(df_grid.W, 3)\n",
    "df_grid.Q2 = np.round(df_grid.Q2, 3)\n",
    "df_grid.cos_theta = np.round(df_grid.cos_theta, 3)\n",
    "df_grid.phi = np.round(df_grid.phi, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
