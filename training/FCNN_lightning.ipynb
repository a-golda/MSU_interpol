{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d9743c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31ee66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5019422",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:0px !important;margin-left:-30px !important; } .container { width:100% !important; } .end_space { min-height:0px !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make screen wide\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython import get_ipython as get_ipython\n",
    "display(HTML(\n",
    "    '<style>'\n",
    "        '#notebook { padding-top:0px !important;margin-left:-30px !important; } ' \n",
    "        '.container { width:100% !important; } '\n",
    "        '.end_space { min-height:0px !important; } '\n",
    "    '</style>'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrongly displayed params in wandb UI\n",
    "# set up dynamic LR\n",
    "# how to run the same wandb experiment from checkpoint\n",
    "# val_split is it static ??? Where to split data (is it one split in validation???)\n",
    "# write logs after each epoch -  probably just one more self.log(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386f7d9",
   "metadata": {},
   "source": [
    "# FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa60e954",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#params\n",
    "logger_path = './wandb_local_logs'\n",
    "data_path = '../data/clasdb_pi_plus_n.txt'\n",
    "\n",
    "project_name = \"MSU_interpol\"\n",
    "\n",
    "scale_data = False\n",
    "test_size = 0.1\n",
    "batch_size = 32\n",
    "net_architecture = [5, 60, 80, 100, 120, 140, 240, 340, 440, 640, 2000, 1040, 640, 340, 240, 140, 100, 80, 60, 20, 1]\n",
    "lr = 0.00001\n",
    "activation_function = nn.ReLU()\n",
    "loss_func = RMSELoss()\n",
    "optim_func = torch.optim.Adam\n",
    "max_epochs = 2000\n",
    "min_delta = 0.0001\n",
    "patience = 20\n",
    "\n",
    "hyperparams_dict = {\n",
    "    'scale_data': scale_data,\n",
    "    'test_size': test_size,\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "    'net_architecture': net_architecture,\n",
    "    'activation_function': activation_function,\n",
    "    'loss_func': loss_func,\n",
    "    'optim_func': optim_func,\n",
    "    'max_epochs': max_epochs,\n",
    "    'min_delta': min_delta,\n",
    "    'patience': patience\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2256a296",
   "metadata": {
    "code_folding": [
     0,
     14,
     57
    ]
   },
   "outputs": [],
   "source": [
    "class InterpolDataSet(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.len = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class InterpolDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.df = None\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        # data reading and preprocessing\n",
    "        df = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "        df.columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'dsigma_dOmega', 'error', 'id']\n",
    "\n",
    "        df.loc[8314:65671, 'Ebeam'] = 5.754 # peculiarity of this dataset.\n",
    "        df['phi'] = df.phi.apply(lambda x: math.radians(x))\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.iloc[df[['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']].drop_duplicates().index]\n",
    "        self.df = df\n",
    "        \n",
    "        #train test split\n",
    "        feature_data = df[['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']]\n",
    "        label_data = df['dsigma_dOmega']\n",
    "        \n",
    "        if scale_data:\n",
    "            scaler_feature = StandardScaler()\n",
    "            scaler_target = StandardScaler()\n",
    "            feature_data = scaler_feature.fit_transform(feature_data)\n",
    "            label_data = scaler_target.fit_transform(label_data.values.reshape(-1,1))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        train_feature_data, val_feature_data, train_label_data, val_label_data = train_test_split(feature_data, label_data, \n",
    "                                                                                                  test_size=test_size, random_state=1438)\n",
    "        \n",
    "        \n",
    "        self.train_dataset = InterpolDataSet(torch.tensor(train_feature_data.values, dtype=torch.float32), \n",
    "                                             torch.tensor(train_label_data.values, dtype=torch.float32))\n",
    "        \n",
    "        self.val_dataset = InterpolDataSet(torch.tensor(val_feature_data.values, dtype=torch.float32), \n",
    "                                            torch.tensor(val_label_data.values, dtype=torch.float32))\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.train_dataset, batch_size = batch_size, shuffle = False, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.val_dataset, batch_size = batch_size, shuffle = False, num_workers=0)\n",
    "    \n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3af1a1d3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class InterpolRegressor(pl.LightningModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super(InterpolRegressor, self).__init__()\n",
    "        \n",
    "        self.hyperparams = hyperparams\n",
    "        self.save_hyperparameters(self.hyperparams)\n",
    "        \n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.loss_func = self.hyperparams.get('loss_func')\n",
    "        \n",
    "        self.optim = self.hyperparams.get('optim_func')\n",
    "        \n",
    "        self.net_architecture = self.hyperparams.get('net_architecture')\n",
    "        self.activation_function = self.hyperparams.get('activation_function')\n",
    "        \n",
    "        self.net = nn.Sequential()              \n",
    "        for i in range(1,len(self.net_architecture)):\n",
    "            self.net.append(nn.Linear(self.net_architecture[i-1], self.net_architecture[i]))\n",
    "            if i!=len(self.net_architecture)-1:\n",
    "                self.net.append(self.activation_function)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        loss = self.loss_func\n",
    "        self.train_loss = loss(y_hat.reshape(-1), y)\n",
    "        \n",
    "        self.mae_train = self.mae(y_hat.reshape(-1), y)\n",
    "        \n",
    "        self.log('train_loss', self.train_loss, batch_size=self.hyperparams['batch_size'], \n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('train_mae', self.mae_train, batch_size=self.hyperparams['batch_size'], \n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        return self.train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        loss = self.loss_func\n",
    "        self.val_loss = loss(y_hat.reshape(-1), y)\n",
    "        \n",
    "        self.mae_val = self.mae(y_hat.reshape(-1), y)\n",
    "        \n",
    "        self.log('val_loss', self.val_loss, batch_size=self.hyperparams['batch_size'], \n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('val_mae', self.mae_val, batch_size=self.hyperparams['batch_size'], \n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        return self.val_loss\n",
    "\n",
    "    #     def on_train_epoch_end(self):\n",
    "    #         self.log('train_loss', self.train_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "    #     def on_validation_epoch_end(self):\n",
    "    #         self.log('step', self.trainer.current_epoch)\n",
    "    #         self.log('val_loss', self.val_loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optim(self.parameters(), lr=self.hyperparams.get('lr'))\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de32f43c",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a237ca5daf4389a4af0e78c97941c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011161336433401125, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb_local_logs/wandb/run-20240110_000913-7te07qok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/msu_ai/msu_interpol/runs/7te07qok' target=\"_blank\">glad-mountain-6</a></strong> to <a href='https://wandb.ai/msu_ai/msu_interpol' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/msu_ai/msu_interpol' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/msu_ai/msu_interpol/runs/7te07qok' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol/runs/7te07qok</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey.golda/.pyenv/versions/3.9.0/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'activation_function' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['activation_function'])`.\n",
      "/Users/andrey.golda/.pyenv/versions/3.9.0/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/andrey.golda/.pyenv/versions/3.9.0/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:43: attribute 'loss_func' removed from hparams because it cannot be pickled\n",
      "\n",
      "  | Name                | Type              | Params\n",
      "----------------------------------------------------------\n",
      "0 | mae                 | MeanAbsoluteError | 0     \n",
      "1 | loss_func           | RMSELoss          | 0     \n",
      "2 | activation_function | ReLU              | 0     \n",
      "3 | net                 | Sequential        | 5.0 M \n",
      "----------------------------------------------------------\n",
      "5.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 M     Total params\n",
      "19.926    Total estimated model params size (MB)\n",
      "/Users/andrey.golda/.pyenv/versions/3.9.0/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/andrey.golda/.pyenv/versions/3.9.0/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/andrey.golda/.pyenv/versions/3.9.0/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "data_module = InterpolDataModule()\n",
    "\n",
    "wandb_logger = WandbLogger(project=project_name, \n",
    "                           save_dir=logger_path,)\n",
    "exp_name = wandb_logger.experiment.name\n",
    "\n",
    "model = InterpolRegressor(hyperparams=hyperparams_dict)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=min_delta, patience=patience, verbose=True)\n",
    "\n",
    "checkpoint_l1 = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=f\"{logger_path}/{project_name}/{exp_name}/checkpoints\",\n",
    "    filename=\"{exp_name}{val_loss:.5f}-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs,\n",
    "                     accelerator='cpu',\n",
    "#                      callbacks=[early_stop_callback, checkpoint_l1],\n",
    "                     callbacks=[checkpoint_l1],\n",
    "                     logger=wandb_logger,\n",
    "                     enable_progress_bar=False)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0ce0aa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.024 MB uploaded (0.008 MB deduped)\\r'), FloatProgress(value=0.498611…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 34.4%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▇▇▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▇▆▆▅▅▄▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▇▇▆▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>327</td></tr><tr><td>train_loss</td><td>0.33249</td></tr><tr><td>train_mae</td><td>0.16894</td></tr><tr><td>trainer/global_step</td><td>861983</td></tr><tr><td>val_loss</td><td>0.34324</td></tr><tr><td>val_mae</td><td>0.17369</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-mountain-6</strong> at: <a href='https://wandb.ai/msu_ai/msu_interpol/runs/7te07qok' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol/runs/7te07qok</a><br/> View job at <a href='https://wandb.ai/msu_ai/msu_interpol/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNzU0MzY3Mw==/version_details/v4' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNzU0MzY3Mw==/version_details/v4</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb_local_logs/wandb/run-20240110_000913-7te07qok/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c3a2fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.16783376\n"
     ]
    }
   ],
   "source": [
    "preds = model.forward(data_module.train_dataset.features)\n",
    "print('', mean_squared_error(data_module.train_dataset.labels, preds.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0b09090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16841991"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.forward(data_module.val_dataset.features)\n",
    "mean_squared_error(data_module.val_dataset.labels, preds.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c5c038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ac7a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scale_data': False, 'test_size': 0.1, 'batch_size': 16, 'lr': 5e-05, 'net_architecture': [5, 60, 80, 100, 120, 140, 140, 140, 240, 340, 440, 1040, 640, 340, 240, 140, 100, 80, 60, 20, 1], 'activation_function': ReLU(), 'optim_func': 'ADAM', 'max_epochs': 2000, 'min_delta': 0.0001, 'patience': 20}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./wandb_local_logs/MSU_interpol/still-frost-4/val_loss=0.37239-epoch=15.ckpt', map_location=lambda storage, loc: storage)\n",
    "print(checkpoint[\"hyper_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3546dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefd411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5178f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c4317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0f920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367727b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034760b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d904ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
