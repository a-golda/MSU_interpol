{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce34e31",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-04-10T11:59:04.131914Z",
     "start_time": "2024-04-10T11:59:04.114448Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import Callback, LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T11:59:04.646459Z",
     "start_time": "2024-04-10T11:59:04.627205Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FCNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#params\n",
    "project_name = \"MSU_interpol\"\n",
    "\n",
    "logger_path = './wandb_local_logs'\n",
    "data_path = '../data/clasdb_pi_plus_n.txt'\n",
    "\n",
    "hyperparams_dict = {\n",
    "    'scale_data': False,\n",
    "    'augment': False,\n",
    "    'add_abc': True,\n",
    "    'abc_loss_factor': 0.5,\n",
    "    'augment_factor': 20,\n",
    "    'test_size': 0.1,\n",
    "    'batch_size': 256,\n",
    "    'net_architecture': [5,60,80,100,120,140,240,340,440,640,2000,1040,640,340,240,140,100,80,60,20,1],\n",
    "    'activation_function': nn.ReLU(),\n",
    "    'loss_func': 'RMSELoss()',\n",
    "    'optim_func': torch.optim.Adam,\n",
    "    'max_epochs': 2000,\n",
    "    'es_min_delta': 0.00001,\n",
    "    'es_patience': 150,\n",
    "    'lr': 0.001,\n",
    "    'lr_factor':0.5,\n",
    "    'lr_patience': 5,\n",
    "    'lr_cooldown': 20,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T12:04:22.523385Z",
     "start_time": "2024-04-10T12:04:22.505266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_cos(x, a, b, c):\n",
    "        return a + b*torch.cos(2*x) + c*torch.cos(x)\n",
    "\n",
    "    def forward(self,x,y,w,A,B,C):\n",
    "        criterion = torch.sqrt(torch.mean(w*(x - y)**2)/torch.sum(w)) + \\\n",
    "                    torch.mul(hyperparams_dict.get('abc_loss_factor'), torch.mean(torch.abs(y - self.func_cos(x,A,B,C))))\n",
    "        return criterion\n",
    "\n",
    "global_losss_function = RMSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T12:04:24.499240Z",
     "start_time": "2024-04-10T12:04:24.480369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey.golda/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=project_name,\n",
    "                           save_dir=logger_path)\n",
    "exp_name = wandb_logger.experiment.name\n",
    "\n",
    "logger_full_path = os.path.join(logger_path, project_name, exp_name)\n",
    "\n",
    "os.makedirs(logger_full_path, exist_ok=True)\n",
    "logging.basicConfig(encoding='utf-8',\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    handlers=[logging.FileHandler(os.path.join(logger_full_path, 'logs.log'), mode='w'),\n",
    "                              logging.StreamHandler(sys.stdout)],\n",
    "                    force=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T12:04:25.119540Z",
     "start_time": "2024-04-10T12:04:25.095632Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a4c8bf4",
   "metadata": {
    "code_folding": [
     0,
     14,
     57
    ],
    "ExecuteTime": {
     "end_time": "2024-04-10T12:04:26.385403Z",
     "start_time": "2024-04-10T12:04:26.286568Z"
    }
   },
   "outputs": [],
   "source": [
    "class InterpolDataSet(Dataset):\n",
    "    def __init__(self, features, labels, weights, A, B, C):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.len = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        weights = self.weights[index]\n",
    "        A = self.A[index]\n",
    "        B = self.B[index]\n",
    "        C = self.C[index]\n",
    "        return feature, label, weights, A, B, C\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class InterpolDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super().__init__()\n",
    "        self.df = None\n",
    "        self.hyperparams = hyperparams\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def augment(self, new_augm):\n",
    "        augm = pd.Series({'Ebeam': np.random.normal(loc=new_augm.Ebeam, scale=new_augm.Ebeam/30),\n",
    "                           'W': np.random.normal(loc=new_augm.W, scale=new_augm.W/30),\n",
    "                           'Q2': np.random.normal(loc=new_augm.Q2, scale=new_augm.Q2/30),\n",
    "                           'cos_theta': np.clip(np.random.normal(loc=new_augm.cos_theta, scale=abs(new_augm.cos_theta/30)), -1, 1),\n",
    "                           'phi': np.clip(np.random.normal(loc=new_augm.phi, scale=new_augm.phi/30), 0, 2*np.pi),\n",
    "                           'dsigma_dOmega': np.random.normal(loc=new_augm.dsigma_dOmega, scale=new_augm.error/3),\n",
    "                           'error': new_augm.error,\n",
    "                           'weight': new_augm.weight,\n",
    "                          })\n",
    "        if self.hyperparams.get('add_abc'):\n",
    "            augm['A'] = new_augm.A\n",
    "            augm['B'] = new_augm.B\n",
    "            augm['C'] = new_augm.C\n",
    "        else:\n",
    "            pass\n",
    "        return augm\n",
    "\n",
    "    @staticmethod\n",
    "    def func_cos(x, a, b, c):\n",
    "            return a + b*np.cos(2*x) + c*np.cos(x)\n",
    "\n",
    "    def get_abc(self, df, E_beam, Q2, W, cos_theta):\n",
    "        df_example_set = df[(df.Ebeam == E_beam)&\n",
    "                            (df.W == W)&\n",
    "                            (df.Q2 == Q2)&\n",
    "                            (df.cos_theta == cos_theta)].sort_values('phi')\n",
    "        #input data\n",
    "        xdata = df_example_set.phi\n",
    "        ydata = df_example_set.dsigma_dOmega\n",
    "        ydata_error = df_example_set.error\n",
    "        #fitting the data\n",
    "        popt, pcov = curve_fit(self.func_cos, xdata, ydata, sigma=ydata_error, absolute_sigma=True)\n",
    "        a, b, c = popt[0], popt[1], popt[2]\n",
    "\n",
    "        return a, b, c\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        # data reading and preprocessing\n",
    "        df = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "        df.columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'dsigma_dOmega', 'error', 'id']\n",
    "        df.loc[8314:65671, 'Ebeam'] = 5.754 # peculiarity of this dataset.\n",
    "        df['phi'] = df.phi.apply(lambda x: math.radians(x))\n",
    "        df['weight'] = df['error'].apply(lambda x: x and 1 / x or 100) # x and 1 / x or 100  is just a reversed error but with validation 1/0 error in this case it will return 100\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.iloc[df[['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']].drop_duplicates().index]\n",
    "\n",
    "        # #train test split\n",
    "        feature_columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']\n",
    "        #\n",
    "        if self.hyperparams.get('add_abc'):\n",
    "            df['A'] = None\n",
    "            df['B'] = None\n",
    "            df['C'] = None\n",
    "            feature_columns_with_additional = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'weight', 'A', 'B', 'C']\n",
    "        else:\n",
    "            feature_columns_with_additional = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'weight']\n",
    "\n",
    "\n",
    "        if self.hyperparams.get('add_abc'):\n",
    "            for Ebeam in df.Ebeam.unique():\n",
    "                for Q2 in tqdm.tqdm(df[df.Ebeam == Ebeam].Q2.unique(), desc='ABC Q cycle'):\n",
    "                    for W in df[(df.Ebeam == Ebeam) & (df.Q2 == Q2)].W.unique():\n",
    "                        for cos_theta in df[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W)].cos_theta.unique():\n",
    "                            try:\n",
    "                                if df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'A'].iloc[0] is None:\n",
    "                                    A, B, C = self.get_abc(df, Ebeam, Q2, W, cos_theta)\n",
    "                                    df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'A'] = A\n",
    "                                    df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'B'] = B\n",
    "                                    df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'C'] = C\n",
    "                                else:\n",
    "                                    pass\n",
    "                            except Exception as e:\n",
    "                                df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'A'] = 0\n",
    "                                df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'B'] = 0\n",
    "                                df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'C'] = 0\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        feature_data = df[feature_columns_with_additional]\n",
    "        label_data = df['dsigma_dOmega']\n",
    "\n",
    "        if self.hyperparams.get('scale_data'):\n",
    "            scaler_feature = StandardScaler()\n",
    "            scaler_target = StandardScaler()\n",
    "            feature_data = scaler_feature.fit_transform(feature_data)\n",
    "            label_data = scaler_target.fit_transform(label_data.values.reshape(-1,1))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if self.hyperparams.get('augment'):\n",
    "            aug_series_list = []\n",
    "            for i in tqdm.tqdm(df.itertuples()):\n",
    "                for _ in range(self.hyperparams.get('augment_factor')):\n",
    "                    aug_series_list.append(self.augment(i))\n",
    "\n",
    "            aug_df = pd.DataFrame(aug_series_list)\n",
    "            df = pd.concat([df, aug_df])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        train_feature_data, val_feature_data, train_label_data, val_label_data = train_test_split(feature_data,\n",
    "                                                                                                  label_data,\n",
    "                                                                                                  test_size=self.hyperparams.get('test_size'),\n",
    "                                                                                                  random_state=1438)\n",
    "\n",
    "        self.train_dataset = InterpolDataSet(torch.tensor(train_feature_data[feature_columns].values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_label_data.values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['weight'].values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['A'].astype(float).values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['B'].astype(float).values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['C'].astype(float).values, dtype=torch.float32))\n",
    "        \n",
    "        self.val_dataset = InterpolDataSet(torch.tensor(val_feature_data[feature_columns].values, dtype=torch.float32),\n",
    "                                           torch.tensor(val_label_data.values, dtype=torch.float32),\n",
    "                                           torch.tensor(val_feature_data['weight'].values, dtype=torch.float32),\n",
    "                                           torch.tensor(train_feature_data['A'].astype(float).values, dtype=torch.float32),\n",
    "                                           torch.tensor(train_feature_data['B'].astype(float).values, dtype=torch.float32),\n",
    "                                           torch.tensor(train_feature_data['C'].astype(float).values, dtype=torch.float32))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.train_dataset, batch_size = self.hyperparams.get('batch_size'), shuffle = False, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.val_dataset, batch_size = self.hyperparams.get('batch_size'), shuffle = False, num_workers=0)\n",
    "\n",
    "class PrintCallbacks(Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        logging.info(\"Training is starting\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        logging.info(\"Training is ending\")\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\n",
    "        logging.info(f\"epoch: {pl_module.current_epoch}; train_loss: {epoch_mean}\")\n",
    "        pl_module.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch_mean = torch.stack(pl_module.validation_step_outputs).mean()\n",
    "        logging.info(f\"epoch: {pl_module.current_epoch}; val_loss: {epoch_mean}\")\n",
    "        pl_module.validation_step_outputs.clear()\n",
    "\n",
    "class InterpolRegressor(pl.LightningModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super(InterpolRegressor, self).__init__()\n",
    "\n",
    "        self.train_loss, self.train_mae, self.val_loss, self.val_mae = 0,0,0,0\n",
    "        self.hyperparams = hyperparams\n",
    "        self.save_hyperparameters(self.hyperparams)\n",
    "\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.loss_func = global_losss_function\n",
    "\n",
    "        self.optim = self.hyperparams.get('optim_func')\n",
    "\n",
    "        self.net_architecture = self.hyperparams.get('net_architecture')\n",
    "        self.activation_function = self.hyperparams.get('activation_function')\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(1,len(self.net_architecture)):\n",
    "            self.net.append(nn.Linear(self.net_architecture[i-1], self.net_architecture[i]))\n",
    "            if i!=len(self.net_architecture)-1:\n",
    "                self.net.append(self.activation_function)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, w, A, B, C = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_func\n",
    "        self.train_loss = loss.forward(y_hat.reshape(-1), y, w, A, B, C)\n",
    "        self.train_mae = self.mae(y_hat.reshape(-1), y)\n",
    "\n",
    "        self.log('train_loss', self.train_loss, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('train_mae', self.train_mae, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "\n",
    "        self.training_step_outputs.append(self.train_loss)\n",
    "        return self.train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, w, A, B, C  = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_func\n",
    "        self.val_loss = loss.forward(y_hat.reshape(-1), y, w, A, B, C)\n",
    "        self.val_mae = self.mae(y_hat.reshape(-1), y)\n",
    "\n",
    "        self.log('val_loss', self.val_loss, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('val_mae', self.val_mae, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "\n",
    "        self.validation_step_outputs.append(self.val_loss)\n",
    "        return self.val_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        sch = self.lr_schedulers()\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau) and self.trainer.current_epoch!=0:\n",
    "                sch.step(self.trainer.callback_metrics[\"val_loss\"])\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                            min_delta=self.hyperparams.get('es_min_delta'),\n",
    "                                            patience=self.hyperparams.get('es_patience'),\n",
    "                                            verbose=True)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(save_top_k=3,\n",
    "                                              monitor=\"val_loss\",\n",
    "                                              mode=\"min\",\n",
    "                                              dirpath=f\"{logger_full_path}/checkpoints\",\n",
    "                                              filename=\"{exp_name}{val_loss:.5f}-{epoch:02d}\")\n",
    "\n",
    "        lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "        print_callback = PrintCallbacks()\n",
    "\n",
    "        return [early_stop_callback, checkpoint_callback, print_callback, lr_monitor]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optim(self.parameters(), lr=self.hyperparams.get('lr'))\n",
    "        lr_optim = ReduceLROnPlateau(optimizer = optimizer,\n",
    "                                     mode = 'min',\n",
    "                                     factor = self.hyperparams.get('lr_factor'),\n",
    "                                     patience = self.hyperparams.get('lr_patience'),\n",
    "                                     cooldown=self.hyperparams.get('lr_cooldown'),\n",
    "                                     threshold=0.01,\n",
    "                                     verbose= True)\n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": lr_optim,\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"frequency\": 2,\n",
    "                    \"name\": 'lr_scheduler_monitoring'}\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b5babb2",
   "metadata": {
    "code_folding": [],
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T13:00:40.299592Z",
     "start_time": "2024-04-10T13:00:40.292157Z"
    }
   },
   "outputs": [],
   "source": [
    "data_module = InterpolDataModule(hyperparams=hyperparams_dict)\n",
    "model = InterpolRegressor(hyperparams=hyperparams_dict)\n",
    "# model = InterpolRegressor.load_from_checkpoint(f'./wandb_local_logs/MSU_interpol/blooming-plasma-40/checkpoints/exp_name=0val_loss=6.43574-epoch=14.ckpt', hyperparams=hyperparams_dict)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=hyperparams_dict.get('max_epochs'),\n",
    "                     accelerator='cpu',\n",
    "                     logger=wandb_logger,\n",
    "                     enable_progress_bar=False)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f33d989",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T12:58:47.583249Z",
     "start_time": "2024-04-10T12:58:37.349945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc1cd02f3e8043059ca8111a6d39c00a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B sync reduced upload amount by 2.1%             "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr_scheduler_monitoring</td><td>████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▄▁██▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>train_mae</td><td>█▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>val_mae</td><td>█▇▇▇▆▆▄▄▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>95</td></tr><tr><td>lr_scheduler_monitoring</td><td>2e-05</td></tr><tr><td>train_loss</td><td>0.03244</td></tr><tr><td>train_mae</td><td>0.06706</td></tr><tr><td>trainer/global_step</td><td>11040</td></tr><tr><td>val_loss</td><td>0.30263</td></tr><tr><td>val_mae</td><td>0.07079</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">robust-shadow-10</strong> at: <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/rs685rti' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/rs685rti</a><br/> View job at <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTI0NDU1Ng==/version_details/v1' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol_by_energy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTI0NDU1Ng==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb_local_logs/wandb/run-20240410_125910-rs685rti/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
