{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce34e31",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-04-18T09:05:27.775611Z",
     "start_time": "2024-04-18T09:05:27.772183Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import Callback, LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:05:27.963757Z",
     "start_time": "2024-04-18T09:05:27.895578Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FCNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#params\n",
    "project_name = \"MSU_interpol_by_energy\"\n",
    "\n",
    "logger_path = './wandb_local_logs'\n",
    "data_path = '../data/clasdb_pi_plus_n.txt'\n",
    "\n",
    "hyperparams_dict = {\n",
    "    'scale_data': False,\n",
    "    'augment': False,\n",
    "    'add_abc': True,\n",
    "    'abc_loss_factor': 0.5,\n",
    "    'augment_factor': 20,\n",
    "    'test_size': 0.1,\n",
    "    'batch_size': 256,\n",
    "    'net_architecture': [5,60,80,100,120,140,240,340,440,640,2000,1040,640,340,240,140,100,80,60,20,1],\n",
    "    'activation_function': nn.ReLU(),\n",
    "    'loss_func': 'RMSELoss()',\n",
    "    'optim_func': torch.optim.Adam,\n",
    "    'max_epochs': 2000,\n",
    "    'es_min_delta': 0.00001,\n",
    "    'es_patience': 150,\n",
    "    'lr': 0.001,\n",
    "    'lr_factor':0.5,\n",
    "    'lr_patience': 5,\n",
    "    'lr_cooldown': 20,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:05:28.790066Z",
     "start_time": "2024-04-18T09:05:28.786185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_cos(x, a, b, c):\n",
    "        return a + b*torch.cos(2*x) + c*torch.cos(x)\n",
    "\n",
    "    def forward(self,x,y,w,A,B,C):\n",
    "        criterion = torch.sqrt(torch.mean(w*(x - y)**2)/torch.sum(w)) + \\\n",
    "                    torch.mul(hyperparams_dict.get('abc_loss_factor'), torch.mean(torch.abs(y - self.func_cos(x,A,B,C))))\n",
    "        return criterion\n",
    "\n",
    "global_losss_function = RMSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:05:29.364351Z",
     "start_time": "2024-04-18T09:05:29.360198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:05:30,770 : DEBUG : Popen(['git', 'cat-file', '--batch-check'], cwd=/Users/andrey.golda/Documents/Study/MSU_interpol, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>./wandb_local_logs/wandb/run-20240418_100530-sfp3r23n</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/sfp3r23n' target=\"_blank\">denim-jazz-12</a></strong> to <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol_by_energy</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/sfp3r23n' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/sfp3r23n</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=project_name,\n",
    "                           save_dir=logger_path)\n",
    "exp_name = wandb_logger.experiment.name\n",
    "\n",
    "logger_full_path = os.path.join(logger_path, project_name, exp_name)\n",
    "\n",
    "os.makedirs(logger_full_path, exist_ok=True)\n",
    "logging.basicConfig(encoding='utf-8',\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    handlers=[logging.FileHandler(os.path.join(logger_full_path, 'logs.log'), mode='w'),\n",
    "                              logging.StreamHandler(sys.stdout)],\n",
    "                    force=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:05:31.761140Z",
     "start_time": "2024-04-18T09:05:30.636130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4c8bf4",
   "metadata": {
    "code_folding": [
     0,
     14,
     57
    ],
    "ExecuteTime": {
     "end_time": "2024-04-18T09:05:34.033992Z",
     "start_time": "2024-04-18T09:05:33.995170Z"
    }
   },
   "outputs": [],
   "source": [
    "class InterpolDataSet(Dataset):\n",
    "    def __init__(self, features, labels, weights, A, B, C):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.len = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        weights = self.weights[index]\n",
    "        A = self.A[index]\n",
    "        B = self.B[index]\n",
    "        C = self.C[index]\n",
    "        return feature, label, weights, A, B, C\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class InterpolDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super().__init__()\n",
    "        self.df = None\n",
    "        self.hyperparams = hyperparams\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def augment(self, new_augm):\n",
    "        augm = pd.Series({'Ebeam': np.random.normal(loc=new_augm.Ebeam, scale=new_augm.Ebeam/30),\n",
    "                           'W': np.random.normal(loc=new_augm.W, scale=new_augm.W/30),\n",
    "                           'Q2': np.random.normal(loc=new_augm.Q2, scale=new_augm.Q2/30),\n",
    "                           'cos_theta': np.clip(np.random.normal(loc=new_augm.cos_theta, scale=abs(new_augm.cos_theta/30)), -1, 1),\n",
    "                           'phi': np.clip(np.random.normal(loc=new_augm.phi, scale=new_augm.phi/30), 0, 2*np.pi),\n",
    "                           'dsigma_dOmega': np.random.normal(loc=new_augm.dsigma_dOmega, scale=new_augm.error/3),\n",
    "                           'error': new_augm.error,\n",
    "                           'weight': new_augm.weight,\n",
    "                          })\n",
    "        if self.hyperparams.get('add_abc'):\n",
    "            augm['A'] = new_augm.A\n",
    "            augm['B'] = new_augm.B\n",
    "            augm['C'] = new_augm.C\n",
    "        else:\n",
    "            pass\n",
    "        return augm\n",
    "\n",
    "    @staticmethod\n",
    "    def func_cos(x, a, b, c):\n",
    "            return a + b*np.cos(2*x) + c*np.cos(x)\n",
    "\n",
    "    def get_abc(self, df, E_beam, Q2, W, cos_theta):\n",
    "        df_example_set = df[(df.Ebeam == E_beam)&\n",
    "                            (df.W == W)&\n",
    "                            (df.Q2 == Q2)&\n",
    "                            (df.cos_theta == cos_theta)].sort_values('phi')\n",
    "        #input data\n",
    "        xdata = df_example_set.phi\n",
    "        ydata = df_example_set.dsigma_dOmega\n",
    "        ydata_error = df_example_set.error\n",
    "        #fitting the data\n",
    "        popt, pcov = curve_fit(self.func_cos, xdata, ydata, sigma=ydata_error, absolute_sigma=True)\n",
    "        a, b, c = popt[0], popt[1], popt[2]\n",
    "\n",
    "        return a, b, c\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        # data reading and preprocessing\n",
    "        df = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "        df.columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'dsigma_dOmega', 'error', 'id']\n",
    "        df.loc[8314:65671, 'Ebeam'] = 5.754 # peculiarity of this dataset.\n",
    "        df['phi'] = df.phi.apply(lambda x: math.radians(x))\n",
    "        df['weight'] = df['error'].apply(lambda x: x and 1 / x or 100) # x and 1 / x or 100  is just a reversed error but with validation 1/0 error in this case it will return 100\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.iloc[df[['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']].drop_duplicates().index]\n",
    "\n",
    "        # TODO: critical\n",
    "        # Ebeam = [5.754]\n",
    "        # Q2 = [1.72, 2.05, 2.44, 2.91, 3.48, 4.155]\n",
    "        # df = df[(df.Q2.isin(Q2))&(df.Ebeam.isin(Ebeam))]\n",
    "\n",
    "        Ebeam = [5.499]\n",
    "        W = [1.830, 1.890, 1.780, 1.950, 2.010, 1.620, 1.660, 1.700, 1.740]\n",
    "        df = df[df.Ebeam.isin(Ebeam)&(df.W.isin(W))]\n",
    "\n",
    "        # Ebeam = [1.515]\n",
    "        # df = df[df.Ebeam.isin(Ebeam)]\n",
    "\n",
    "\n",
    "        # #train test split\n",
    "        feature_columns = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi']\n",
    "        #\n",
    "        if self.hyperparams.get('add_abc'):\n",
    "            df['A'] = None\n",
    "            df['B'] = None\n",
    "            df['C'] = None\n",
    "            feature_columns_with_additional = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'weight', 'A', 'B', 'C']\n",
    "        else:\n",
    "            feature_columns_with_additional = ['Ebeam', 'W', 'Q2', 'cos_theta', 'phi', 'weight']\n",
    "\n",
    "\n",
    "        if self.hyperparams.get('add_abc'):\n",
    "            for Ebeam in df.Ebeam.unique():\n",
    "                for Q2 in tqdm.tqdm(df[df.Ebeam == Ebeam].Q2.unique(), desc='ABC Q cycle'):\n",
    "                    for W in df[(df.Ebeam == Ebeam) & (df.Q2 == Q2)].W.unique():\n",
    "                        for cos_theta in df[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W)].cos_theta.unique():\n",
    "                            try:\n",
    "                                if df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'A'].iloc[0] is None:\n",
    "                                    A, B, C = self.get_abc(df, Ebeam, Q2, W, cos_theta)\n",
    "                                    df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'A'] = A\n",
    "                                    df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'B'] = B\n",
    "                                    df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'C'] = C\n",
    "                                else:\n",
    "                                    pass\n",
    "                            except Exception as e:\n",
    "                                df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'A'] = 0\n",
    "                                df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'B'] = 0\n",
    "                                df.loc[(df.Ebeam == Ebeam) & (df.Q2 == Q2) & (df.W == W) & (df.cos_theta == cos_theta), 'C'] = 0\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        feature_data = df[feature_columns_with_additional]\n",
    "        label_data = df['dsigma_dOmega']\n",
    "\n",
    "        if self.hyperparams.get('scale_data'):\n",
    "            scaler_feature = StandardScaler()\n",
    "            scaler_target = StandardScaler()\n",
    "            feature_data = scaler_feature.fit_transform(feature_data)\n",
    "            label_data = scaler_target.fit_transform(label_data.values.reshape(-1,1))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if self.hyperparams.get('augment'):\n",
    "            aug_series_list = []\n",
    "            for i in tqdm.tqdm(df.itertuples()):\n",
    "                for _ in range(self.hyperparams.get('augment_factor')):\n",
    "                    aug_series_list.append(self.augment(i))\n",
    "\n",
    "            aug_df = pd.DataFrame(aug_series_list)\n",
    "            df = pd.concat([df, aug_df])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        train_feature_data, val_feature_data, train_label_data, val_label_data = train_test_split(feature_data,\n",
    "                                                                                                  label_data,\n",
    "                                                                                                  test_size=self.hyperparams.get('test_size'),\n",
    "                                                                                                  random_state=1438)\n",
    "\n",
    "        self.train_dataset = InterpolDataSet(torch.tensor(train_feature_data[feature_columns].values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_label_data.values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['weight'].values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['A'].astype(float).values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['B'].astype(float).values, dtype=torch.float32),\n",
    "                                             torch.tensor(train_feature_data['C'].astype(float).values, dtype=torch.float32))\n",
    "        \n",
    "        self.val_dataset = InterpolDataSet(torch.tensor(val_feature_data[feature_columns].values, dtype=torch.float32),\n",
    "                                           torch.tensor(val_label_data.values, dtype=torch.float32),\n",
    "                                           torch.tensor(val_feature_data['weight'].values, dtype=torch.float32),\n",
    "                                           torch.tensor(train_feature_data['A'].astype(float).values, dtype=torch.float32),\n",
    "                                           torch.tensor(train_feature_data['B'].astype(float).values, dtype=torch.float32),\n",
    "                                           torch.tensor(train_feature_data['C'].astype(float).values, dtype=torch.float32))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.train_dataset, batch_size = self.hyperparams.get('batch_size'), shuffle = False, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.val_dataset, batch_size = self.hyperparams.get('batch_size'), shuffle = False, num_workers=0)\n",
    "\n",
    "class PrintCallbacks(Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        logging.info(\"Training is starting\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        logging.info(\"Training is ending\")\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\n",
    "        logging.info(f\"epoch: {pl_module.current_epoch}; train_loss: {epoch_mean}\")\n",
    "        pl_module.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch_mean = torch.stack(pl_module.validation_step_outputs).mean()\n",
    "        logging.info(f\"epoch: {pl_module.current_epoch}; val_loss: {epoch_mean}\")\n",
    "        pl_module.validation_step_outputs.clear()\n",
    "\n",
    "class InterpolRegressor(pl.LightningModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        super(InterpolRegressor, self).__init__()\n",
    "\n",
    "        self.train_loss, self.train_mae, self.val_loss, self.val_mae = 0,0,0,0\n",
    "        self.hyperparams = hyperparams\n",
    "        self.save_hyperparameters(self.hyperparams)\n",
    "\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.loss_func = global_losss_function\n",
    "\n",
    "        self.optim = self.hyperparams.get('optim_func')\n",
    "\n",
    "        self.net_architecture = self.hyperparams.get('net_architecture')\n",
    "        self.activation_function = self.hyperparams.get('activation_function')\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(1,len(self.net_architecture)):\n",
    "            self.net.append(nn.Linear(self.net_architecture[i-1], self.net_architecture[i]))\n",
    "            if i!=len(self.net_architecture)-1:\n",
    "                self.net.append(self.activation_function)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, w, A, B, C = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_func\n",
    "        self.train_loss = loss.forward(y_hat.reshape(-1), y, w, A, B, C)\n",
    "        self.train_mae = self.mae(y_hat.reshape(-1), y)\n",
    "\n",
    "        self.log('train_loss', self.train_loss, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('train_mae', self.train_mae, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "\n",
    "        self.training_step_outputs.append(self.train_loss)\n",
    "        return self.train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, w, A, B, C  = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_func\n",
    "        self.val_loss = loss.forward(y_hat.reshape(-1), y, w, A, B, C)\n",
    "        self.val_mae = self.mae(y_hat.reshape(-1), y)\n",
    "\n",
    "        self.log('val_loss', self.val_loss, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "        self.log('val_mae', self.val_mae, batch_size=self.hyperparams['batch_size'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True, sync_dist=True, logger=True)\n",
    "\n",
    "        self.validation_step_outputs.append(self.val_loss)\n",
    "        return self.val_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        sch = self.lr_schedulers()\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau) and self.trainer.current_epoch!=0:\n",
    "                sch.step(self.trainer.callback_metrics[\"val_loss\"])\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                            min_delta=self.hyperparams.get('es_min_delta'),\n",
    "                                            patience=self.hyperparams.get('es_patience'),\n",
    "                                            verbose=True)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(save_top_k=3,\n",
    "                                              monitor=\"val_loss\",\n",
    "                                              mode=\"min\",\n",
    "                                              dirpath=f\"{logger_full_path}/checkpoints\",\n",
    "                                              filename=\"{exp_name}{val_loss:.5f}-{epoch:02d}\")\n",
    "\n",
    "        lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "        print_callback = PrintCallbacks()\n",
    "\n",
    "        return [early_stop_callback, checkpoint_callback, print_callback, lr_monitor]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optim(self.parameters(), lr=self.hyperparams.get('lr'))\n",
    "        lr_optim = ReduceLROnPlateau(optimizer = optimizer,\n",
    "                                     mode = 'min',\n",
    "                                     factor = self.hyperparams.get('lr_factor'),\n",
    "                                     patience = self.hyperparams.get('lr_patience'),\n",
    "                                     cooldown=self.hyperparams.get('lr_cooldown'),\n",
    "                                     threshold=0.01,\n",
    "                                     verbose= True)\n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": lr_optim,\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"frequency\": 2,\n",
    "                    \"name\": 'lr_scheduler_monitoring'}\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b5babb2",
   "metadata": {
    "code_folding": [],
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:31:36.176694Z",
     "start_time": "2024-04-18T09:05:35.199064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey.golda/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'activation_function' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['activation_function'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "ABC Q cycle: 100%|██████████| 5/5 [00:07<00:00,  1.58s/it]\n",
      "/Users/andrey.golda/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name                | Type              | Params\n",
      "----------------------------------------------------------\n",
      "0 | mae                 | MeanAbsoluteError | 0     \n",
      "1 | loss_func           | RMSELoss          | 0     \n",
      "2 | activation_function | ReLU              | 0     \n",
      "3 | net                 | Sequential        | 5.0 M \n",
      "----------------------------------------------------------\n",
      "5.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 M     Total params\n",
      "19.926    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:05:43,635 : INFO : epoch: 0; val_loss: 0.21666359901428223\n",
      "2024-04-18 10:05:43,638 : INFO : Training is starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey.golda/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/andrey.golda/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:05:55,498 : INFO : epoch: 0; val_loss: 0.20546846091747284\n",
      "2024-04-18 10:05:55,500 : INFO : epoch: 0; train_loss: 0.1326690912246704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:05:55,573 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20547-epoch=00.ckpt\n",
      "2024-04-18 10:06:07,310 : INFO : epoch: 1; val_loss: 0.2019404172897339\n",
      "2024-04-18 10:06:07,313 : INFO : epoch: 1; train_loss: 0.10382968187332153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 1e-05. New best score: 0.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:06:07,380 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20194-epoch=01.ckpt\n",
      "2024-04-18 10:06:18,725 : INFO : epoch: 2; val_loss: 0.20932719111442566\n",
      "2024-04-18 10:06:18,728 : INFO : epoch: 2; train_loss: 0.10399551689624786\n",
      "2024-04-18 10:06:18,802 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20933-epoch=02.ckpt\n",
      "2024-04-18 10:06:30,750 : INFO : epoch: 3; val_loss: 0.19999602437019348\n",
      "2024-04-18 10:06:30,752 : INFO : epoch: 3; train_loss: 0.097324900329113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 1e-05. New best score: 0.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:06:30,832 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20000-epoch=03.ckpt\n",
      "2024-04-18 10:06:42,624 : INFO : epoch: 4; val_loss: 0.20224277675151825\n",
      "2024-04-18 10:06:42,636 : INFO : epoch: 4; train_loss: 0.09864036738872528\n",
      "2024-04-18 10:06:42,714 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20224-epoch=04.ckpt\n",
      "2024-04-18 10:06:53,939 : INFO : epoch: 5; val_loss: 0.20375403761863708\n",
      "2024-04-18 10:06:53,940 : INFO : epoch: 5; train_loss: 0.09610549360513687\n",
      "2024-04-18 10:07:04,977 : INFO : epoch: 6; val_loss: 0.2047029733657837\n",
      "2024-04-18 10:07:04,979 : INFO : epoch: 6; train_loss: 0.09275542199611664\n",
      "2024-04-18 10:07:15,981 : INFO : epoch: 7; val_loss: 0.20221857726573944\n",
      "2024-04-18 10:07:15,984 : INFO : epoch: 7; train_loss: 0.09186900407075882\n",
      "2024-04-18 10:07:16,051 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20222-epoch=07.ckpt\n",
      "2024-04-18 10:07:27,100 : INFO : epoch: 8; val_loss: 0.2025623917579651\n",
      "2024-04-18 10:07:27,102 : INFO : epoch: 8; train_loss: 0.09012363106012344\n",
      "2024-04-18 10:07:38,351 : INFO : epoch: 9; val_loss: 0.20181825757026672\n",
      "2024-04-18 10:07:38,353 : INFO : epoch: 9; train_loss: 0.08963682502508163\n",
      "2024-04-18 10:07:38,423 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20182-epoch=09.ckpt\n",
      "2024-04-18 10:07:50,429 : INFO : epoch: 10; val_loss: 0.20148314535617828\n",
      "2024-04-18 10:07:50,432 : INFO : epoch: 10; train_loss: 0.08931189775466919\n",
      "2024-04-18 10:07:50,499 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20148-epoch=10.ckpt\n",
      "2024-04-18 10:08:03,410 : INFO : epoch: 11; val_loss: 0.20173951983451843\n",
      "2024-04-18 10:08:03,412 : INFO : epoch: 11; train_loss: 0.08840149641036987\n",
      "2024-04-18 10:08:03,482 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20174-epoch=11.ckpt\n",
      "2024-04-18 10:08:16,824 : INFO : epoch: 12; val_loss: 0.20102889835834503\n",
      "2024-04-18 10:08:16,826 : INFO : epoch: 12; train_loss: 0.08809889107942581\n",
      "2024-04-18 10:08:16,893 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20103-epoch=12.ckpt\n",
      "2024-04-18 10:08:28,675 : INFO : epoch: 13; val_loss: 0.20118726789951324\n",
      "2024-04-18 10:08:28,678 : INFO : epoch: 13; train_loss: 0.08783267438411713\n",
      "2024-04-18 10:08:28,748 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20119-epoch=13.ckpt\n",
      "2024-04-18 10:08:40,517 : INFO : epoch: 14; val_loss: 0.19944851100444794\n",
      "2024-04-18 10:08:40,520 : INFO : epoch: 14; train_loss: 0.08818520605564117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 1e-05. New best score: 0.199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-18 10:08:40,591 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.19945-epoch=14.ckpt\n",
      "2024-04-18 10:08:51,800 : INFO : epoch: 15; val_loss: 0.20109468698501587\n",
      "2024-04-18 10:08:51,802 : INFO : epoch: 15; train_loss: 0.08780191093683243\n",
      "2024-04-18 10:09:02,967 : INFO : epoch: 16; val_loss: 0.20091745257377625\n",
      "2024-04-18 10:09:02,970 : INFO : epoch: 16; train_loss: 0.08745916187763214\n",
      "2024-04-18 10:09:03,032 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.20092-epoch=16.ckpt\n",
      "2024-04-18 10:09:13,944 : INFO : epoch: 17; val_loss: 0.20404188334941864\n",
      "2024-04-18 10:09:13,947 : INFO : epoch: 17; train_loss: 0.08638578653335571\n",
      "2024-04-18 10:09:24,741 : INFO : epoch: 18; val_loss: 0.20284134149551392\n",
      "2024-04-18 10:09:24,744 : INFO : epoch: 18; train_loss: 0.08575736731290817\n",
      "2024-04-18 10:09:35,695 : INFO : epoch: 19; val_loss: 0.19991976022720337\n",
      "2024-04-18 10:09:35,697 : INFO : epoch: 19; train_loss: 0.08610092103481293\n",
      "2024-04-18 10:09:35,766 : DEBUG : open file: /Users/andrey.golda/Documents/Study/MSU_interpol/training/wandb_local_logs/MSU_interpol_by_energy/denim-jazz-12/checkpoints/exp_name=0val_loss=0.19992-epoch=19.ckpt\n",
      "2024-04-18 10:09:46,849 : INFO : epoch: 20; val_loss: 0.20172375440597534\n",
      "2024-04-18 10:09:46,851 : INFO : epoch: 20; train_loss: 0.08615020662546158\n",
      "2024-04-18 10:09:58,053 : INFO : epoch: 21; val_loss: 0.20292216539382935\n",
      "2024-04-18 10:09:58,055 : INFO : epoch: 21; train_loss: 0.08643557131290436\n",
      "2024-04-18 10:10:09,593 : INFO : epoch: 22; val_loss: 0.20357047021389008\n",
      "2024-04-18 10:10:09,595 : INFO : epoch: 22; train_loss: 0.08574075996875763\n",
      "2024-04-18 10:10:21,838 : INFO : epoch: 23; val_loss: 0.20176932215690613\n",
      "2024-04-18 10:10:21,842 : INFO : epoch: 23; train_loss: 0.084914430975914\n",
      "2024-04-18 10:10:33,725 : INFO : epoch: 24; val_loss: 0.2010277509689331\n",
      "2024-04-18 10:10:33,727 : INFO : epoch: 24; train_loss: 0.08517470210790634\n",
      "2024-04-18 10:10:45,944 : INFO : epoch: 25; val_loss: 0.20203076303005219\n",
      "2024-04-18 10:10:45,946 : INFO : epoch: 25; train_loss: 0.08484824746847153\n",
      "2024-04-18 10:10:57,127 : INFO : epoch: 26; val_loss: 0.20162087678909302\n",
      "2024-04-18 10:10:57,129 : INFO : epoch: 26; train_loss: 0.08500314503908157\n",
      "2024-04-18 10:11:08,678 : INFO : epoch: 27; val_loss: 0.2015623152256012\n",
      "2024-04-18 10:11:08,680 : INFO : epoch: 27; train_loss: 0.08449286222457886\n",
      "2024-04-18 10:11:19,882 : INFO : epoch: 28; val_loss: 0.2015257626771927\n",
      "2024-04-18 10:11:19,884 : INFO : epoch: 28; train_loss: 0.08460016548633575\n",
      "2024-04-18 10:11:31,108 : INFO : epoch: 29; val_loss: 0.20120422542095184\n",
      "2024-04-18 10:11:31,110 : INFO : epoch: 29; train_loss: 0.08470545709133148\n",
      "2024-04-18 10:11:42,330 : INFO : epoch: 30; val_loss: 0.2015445977449417\n",
      "2024-04-18 10:11:42,332 : INFO : epoch: 30; train_loss: 0.08447129279375076\n",
      "2024-04-18 10:11:53,558 : INFO : epoch: 31; val_loss: 0.20181553065776825\n",
      "2024-04-18 10:11:53,560 : INFO : epoch: 31; train_loss: 0.08427710086107254\n",
      "2024-04-18 10:12:04,921 : INFO : epoch: 32; val_loss: 0.20109157264232635\n",
      "2024-04-18 10:12:04,924 : INFO : epoch: 32; train_loss: 0.08424265682697296\n",
      "2024-04-18 10:12:16,251 : INFO : epoch: 33; val_loss: 0.20154115557670593\n",
      "2024-04-18 10:12:16,254 : INFO : epoch: 33; train_loss: 0.08416882902383804\n",
      "2024-04-18 10:12:28,319 : INFO : epoch: 34; val_loss: 0.20065295696258545\n",
      "2024-04-18 10:12:28,321 : INFO : epoch: 34; train_loss: 0.08416847139596939\n",
      "2024-04-18 10:12:40,484 : INFO : epoch: 35; val_loss: 0.20364615321159363\n",
      "2024-04-18 10:12:40,488 : INFO : epoch: 35; train_loss: 0.0845283716917038\n",
      "2024-04-18 10:12:53,163 : INFO : epoch: 36; val_loss: 0.20183169841766357\n",
      "2024-04-18 10:12:53,166 : INFO : epoch: 36; train_loss: 0.08452248573303223\n",
      "2024-04-18 10:13:04,817 : INFO : epoch: 37; val_loss: 0.20210081338882446\n",
      "2024-04-18 10:13:04,819 : INFO : epoch: 37; train_loss: 0.08396651595830917\n",
      "2024-04-18 10:13:16,307 : INFO : epoch: 38; val_loss: 0.2018747627735138\n",
      "2024-04-18 10:13:16,309 : INFO : epoch: 38; train_loss: 0.08401580899953842\n",
      "2024-04-18 10:13:28,071 : INFO : epoch: 39; val_loss: 0.20228928327560425\n",
      "2024-04-18 10:13:28,073 : INFO : epoch: 39; train_loss: 0.08387744426727295\n",
      "2024-04-18 10:13:39,267 : INFO : epoch: 40; val_loss: 0.20344989001750946\n",
      "2024-04-18 10:13:39,269 : INFO : epoch: 40; train_loss: 0.08404271304607391\n",
      "2024-04-18 10:13:50,731 : INFO : epoch: 41; val_loss: 0.20354485511779785\n",
      "2024-04-18 10:13:50,733 : INFO : epoch: 41; train_loss: 0.08391886949539185\n",
      "2024-04-18 10:14:02,180 : INFO : epoch: 42; val_loss: 0.20300854742527008\n",
      "2024-04-18 10:14:02,181 : INFO : epoch: 42; train_loss: 0.08382551372051239\n",
      "2024-04-18 10:14:13,691 : INFO : epoch: 43; val_loss: 0.20310840010643005\n",
      "2024-04-18 10:14:13,693 : INFO : epoch: 43; train_loss: 0.0835082083940506\n",
      "2024-04-18 10:14:24,863 : INFO : epoch: 44; val_loss: 0.20340664684772491\n",
      "2024-04-18 10:14:24,865 : INFO : epoch: 44; train_loss: 0.08363868296146393\n",
      "2024-04-18 10:14:35,993 : INFO : epoch: 45; val_loss: 0.20274564623832703\n",
      "2024-04-18 10:14:35,995 : INFO : epoch: 45; train_loss: 0.08366994559764862\n",
      "2024-04-18 10:14:47,296 : INFO : epoch: 46; val_loss: 0.2029440999031067\n",
      "2024-04-18 10:14:47,298 : INFO : epoch: 46; train_loss: 0.08364062011241913\n",
      "2024-04-18 10:14:58,812 : INFO : epoch: 47; val_loss: 0.20287388563156128\n",
      "2024-04-18 10:14:58,814 : INFO : epoch: 47; train_loss: 0.08359833061695099\n",
      "2024-04-18 10:15:10,201 : INFO : epoch: 48; val_loss: 0.20332403481006622\n",
      "2024-04-18 10:15:10,203 : INFO : epoch: 48; train_loss: 0.08361317217350006\n",
      "2024-04-18 10:15:21,741 : INFO : epoch: 49; val_loss: 0.20317739248275757\n",
      "2024-04-18 10:15:21,743 : INFO : epoch: 49; train_loss: 0.08351600915193558\n",
      "2024-04-18 10:15:33,427 : INFO : epoch: 50; val_loss: 0.2031772881746292\n",
      "2024-04-18 10:15:33,431 : INFO : epoch: 50; train_loss: 0.0835033506155014\n",
      "2024-04-18 10:15:44,931 : INFO : epoch: 51; val_loss: 0.20307205617427826\n",
      "2024-04-18 10:15:44,933 : INFO : epoch: 51; train_loss: 0.08342145383358002\n",
      "2024-04-18 10:15:56,533 : INFO : epoch: 52; val_loss: 0.20269158482551575\n",
      "2024-04-18 10:15:56,536 : INFO : epoch: 52; train_loss: 0.08342675864696503\n",
      "2024-04-18 10:16:08,276 : INFO : epoch: 53; val_loss: 0.20272192358970642\n",
      "2024-04-18 10:16:08,278 : INFO : epoch: 53; train_loss: 0.08345992863178253\n",
      "2024-04-18 10:16:20,914 : INFO : epoch: 54; val_loss: 0.20268450677394867\n",
      "2024-04-18 10:16:20,917 : INFO : epoch: 54; train_loss: 0.08339071273803711\n",
      "2024-04-18 10:16:32,425 : INFO : epoch: 55; val_loss: 0.20295904576778412\n",
      "2024-04-18 10:16:32,427 : INFO : epoch: 55; train_loss: 0.08340825140476227\n",
      "2024-04-18 10:16:44,212 : INFO : epoch: 56; val_loss: 0.2030092477798462\n",
      "2024-04-18 10:16:44,214 : INFO : epoch: 56; train_loss: 0.08338845521211624\n",
      "2024-04-18 10:16:55,746 : INFO : epoch: 57; val_loss: 0.20245490968227386\n",
      "2024-04-18 10:16:55,748 : INFO : epoch: 57; train_loss: 0.08336904644966125\n",
      "2024-04-18 10:17:07,067 : INFO : epoch: 58; val_loss: 0.2036518156528473\n",
      "2024-04-18 10:17:07,069 : INFO : epoch: 58; train_loss: 0.08306466788053513\n",
      "2024-04-18 10:17:18,687 : INFO : epoch: 59; val_loss: 0.20383602380752563\n",
      "2024-04-18 10:17:18,690 : INFO : epoch: 59; train_loss: 0.0828842893242836\n",
      "2024-04-18 10:17:30,506 : INFO : epoch: 60; val_loss: 0.2039894014596939\n",
      "2024-04-18 10:17:30,508 : INFO : epoch: 60; train_loss: 0.08283920586109161\n",
      "2024-04-18 10:17:41,719 : INFO : epoch: 61; val_loss: 0.2039663940668106\n",
      "2024-04-18 10:17:41,722 : INFO : epoch: 61; train_loss: 0.08270717412233353\n",
      "2024-04-18 10:17:53,112 : INFO : epoch: 62; val_loss: 0.20374341309070587\n",
      "2024-04-18 10:17:53,114 : INFO : epoch: 62; train_loss: 0.08272970467805862\n",
      "2024-04-18 10:18:04,512 : INFO : epoch: 63; val_loss: 0.20379437506198883\n",
      "2024-04-18 10:18:04,530 : INFO : epoch: 63; train_loss: 0.08270372450351715\n",
      "2024-04-18 10:18:16,166 : INFO : epoch: 64; val_loss: 0.20374423265457153\n",
      "2024-04-18 10:18:16,168 : INFO : epoch: 64; train_loss: 0.08265966922044754\n",
      "2024-04-18 10:18:27,503 : INFO : epoch: 65; val_loss: 0.2037188559770584\n",
      "2024-04-18 10:18:27,505 : INFO : epoch: 65; train_loss: 0.08264458179473877\n",
      "2024-04-18 10:18:38,516 : INFO : epoch: 66; val_loss: 0.20360024273395538\n",
      "2024-04-18 10:18:38,518 : INFO : epoch: 66; train_loss: 0.08262132108211517\n",
      "2024-04-18 10:18:49,910 : INFO : epoch: 67; val_loss: 0.20349323749542236\n",
      "2024-04-18 10:18:49,913 : INFO : epoch: 67; train_loss: 0.08267290890216827\n",
      "2024-04-18 10:19:01,321 : INFO : epoch: 68; val_loss: 0.2037065327167511\n",
      "2024-04-18 10:19:01,324 : INFO : epoch: 68; train_loss: 0.0826122909784317\n",
      "2024-04-18 10:19:13,237 : INFO : epoch: 69; val_loss: 0.20366191864013672\n",
      "2024-04-18 10:19:13,240 : INFO : epoch: 69; train_loss: 0.08258669078350067\n",
      "2024-04-18 10:19:24,690 : INFO : epoch: 70; val_loss: 0.20364415645599365\n",
      "2024-04-18 10:19:24,692 : INFO : epoch: 70; train_loss: 0.08252858370542526\n",
      "2024-04-18 10:19:36,051 : INFO : epoch: 71; val_loss: 0.2036975920200348\n",
      "2024-04-18 10:19:36,054 : INFO : epoch: 71; train_loss: 0.08253496140241623\n",
      "2024-04-18 10:19:46,958 : INFO : epoch: 72; val_loss: 0.2033984512090683\n",
      "2024-04-18 10:19:46,960 : INFO : epoch: 72; train_loss: 0.08247260004281998\n",
      "2024-04-18 10:19:57,837 : INFO : epoch: 73; val_loss: 0.20358949899673462\n",
      "2024-04-18 10:19:57,839 : INFO : epoch: 73; train_loss: 0.08245627582073212\n",
      "2024-04-18 10:20:08,204 : INFO : epoch: 74; val_loss: 0.20358408987522125\n",
      "2024-04-18 10:20:08,207 : INFO : epoch: 74; train_loss: 0.08242695033550262\n",
      "2024-04-18 10:20:18,720 : INFO : epoch: 75; val_loss: 0.204514279961586\n",
      "2024-04-18 10:20:18,722 : INFO : epoch: 75; train_loss: 0.08234969526529312\n",
      "2024-04-18 10:20:29,107 : INFO : epoch: 76; val_loss: 0.20448791980743408\n",
      "2024-04-18 10:20:29,109 : INFO : epoch: 76; train_loss: 0.08234180510044098\n",
      "2024-04-18 10:20:39,521 : INFO : epoch: 77; val_loss: 0.20434986054897308\n",
      "2024-04-18 10:20:39,523 : INFO : epoch: 77; train_loss: 0.08230255544185638\n",
      "2024-04-18 10:20:49,808 : INFO : epoch: 78; val_loss: 0.20444324612617493\n",
      "2024-04-18 10:20:49,810 : INFO : epoch: 78; train_loss: 0.0822831317782402\n",
      "2024-04-18 10:21:00,181 : INFO : epoch: 79; val_loss: 0.20440438389778137\n",
      "2024-04-18 10:21:00,183 : INFO : epoch: 79; train_loss: 0.08227897435426712\n",
      "2024-04-18 10:21:12,739 : INFO : epoch: 80; val_loss: 0.20447669923305511\n",
      "2024-04-18 10:21:12,742 : INFO : epoch: 80; train_loss: 0.0822482630610466\n",
      "2024-04-18 10:21:23,842 : INFO : epoch: 81; val_loss: 0.2045341283082962\n",
      "2024-04-18 10:21:23,844 : INFO : epoch: 81; train_loss: 0.08229128271341324\n",
      "2024-04-18 10:21:34,844 : INFO : epoch: 82; val_loss: 0.2045590877532959\n",
      "2024-04-18 10:21:34,846 : INFO : epoch: 82; train_loss: 0.08224889636039734\n",
      "2024-04-18 10:21:45,818 : INFO : epoch: 83; val_loss: 0.20445775985717773\n",
      "2024-04-18 10:21:45,820 : INFO : epoch: 83; train_loss: 0.08226285874843597\n",
      "2024-04-18 10:21:56,954 : INFO : epoch: 84; val_loss: 0.20442995429039001\n",
      "2024-04-18 10:21:56,956 : INFO : epoch: 84; train_loss: 0.08221153914928436\n",
      "2024-04-18 10:22:08,362 : INFO : epoch: 85; val_loss: 0.20454992353916168\n",
      "2024-04-18 10:22:08,364 : INFO : epoch: 85; train_loss: 0.08217920362949371\n",
      "2024-04-18 10:22:19,855 : INFO : epoch: 86; val_loss: 0.20432168245315552\n",
      "2024-04-18 10:22:19,857 : INFO : epoch: 86; train_loss: 0.08216551691293716\n",
      "2024-04-18 10:22:31,662 : INFO : epoch: 87; val_loss: 0.20459379255771637\n",
      "2024-04-18 10:22:31,665 : INFO : epoch: 87; train_loss: 0.08217436075210571\n",
      "2024-04-18 10:22:42,747 : INFO : epoch: 88; val_loss: 0.20439134538173676\n",
      "2024-04-18 10:22:42,749 : INFO : epoch: 88; train_loss: 0.08219189196825027\n",
      "2024-04-18 10:22:54,648 : INFO : epoch: 89; val_loss: 0.20450973510742188\n",
      "2024-04-18 10:22:54,651 : INFO : epoch: 89; train_loss: 0.08214812725782394\n",
      "2024-04-18 10:23:05,725 : INFO : epoch: 90; val_loss: 0.20442993938922882\n",
      "2024-04-18 10:23:05,727 : INFO : epoch: 90; train_loss: 0.08214610069990158\n",
      "2024-04-18 10:23:17,030 : INFO : epoch: 91; val_loss: 0.20467044413089752\n",
      "2024-04-18 10:23:17,032 : INFO : epoch: 91; train_loss: 0.08212611824274063\n",
      "2024-04-18 10:23:28,157 : INFO : epoch: 92; val_loss: 0.20429325103759766\n",
      "2024-04-18 10:23:28,159 : INFO : epoch: 92; train_loss: 0.08200734853744507\n",
      "2024-04-18 10:23:39,333 : INFO : epoch: 93; val_loss: 0.20420604944229126\n",
      "2024-04-18 10:23:39,337 : INFO : epoch: 93; train_loss: 0.0819549486041069\n",
      "2024-04-18 10:23:50,458 : INFO : epoch: 94; val_loss: 0.20424383878707886\n",
      "2024-04-18 10:23:50,461 : INFO : epoch: 94; train_loss: 0.08195290714502335\n",
      "2024-04-18 10:24:01,506 : INFO : epoch: 95; val_loss: 0.20420461893081665\n",
      "2024-04-18 10:24:01,508 : INFO : epoch: 95; train_loss: 0.08193879574537277\n",
      "2024-04-18 10:24:12,690 : INFO : epoch: 96; val_loss: 0.2041947990655899\n",
      "2024-04-18 10:24:12,692 : INFO : epoch: 96; train_loss: 0.08193036168813705\n",
      "2024-04-18 10:24:23,825 : INFO : epoch: 97; val_loss: 0.20415805280208588\n",
      "2024-04-18 10:24:23,827 : INFO : epoch: 97; train_loss: 0.08193051815032959\n",
      "2024-04-18 10:24:35,299 : INFO : epoch: 98; val_loss: 0.2042112499475479\n",
      "2024-04-18 10:24:35,301 : INFO : epoch: 98; train_loss: 0.08191218227148056\n",
      "2024-04-18 10:24:48,617 : INFO : epoch: 99; val_loss: 0.20429545640945435\n",
      "2024-04-18 10:24:48,620 : INFO : epoch: 99; train_loss: 0.08192835003137589\n",
      "2024-04-18 10:25:00,652 : INFO : epoch: 100; val_loss: 0.20418418943881989\n",
      "2024-04-18 10:25:00,655 : INFO : epoch: 100; train_loss: 0.08191056549549103\n",
      "2024-04-18 10:25:12,895 : INFO : epoch: 101; val_loss: 0.2042662799358368\n",
      "2024-04-18 10:25:12,898 : INFO : epoch: 101; train_loss: 0.08192720264196396\n",
      "2024-04-18 10:25:26,464 : INFO : epoch: 102; val_loss: 0.2042027860879898\n",
      "2024-04-18 10:25:26,467 : INFO : epoch: 102; train_loss: 0.08189041912555695\n",
      "2024-04-18 10:25:39,818 : INFO : epoch: 103; val_loss: 0.20423874258995056\n",
      "2024-04-18 10:25:39,820 : INFO : epoch: 103; train_loss: 0.08190137892961502\n",
      "2024-04-18 10:25:53,794 : INFO : epoch: 104; val_loss: 0.20420295000076294\n",
      "2024-04-18 10:25:53,797 : INFO : epoch: 104; train_loss: 0.08190104365348816\n",
      "2024-04-18 10:26:06,945 : INFO : epoch: 105; val_loss: 0.20409446954727173\n",
      "2024-04-18 10:26:06,947 : INFO : epoch: 105; train_loss: 0.08187033236026764\n",
      "2024-04-18 10:26:20,615 : INFO : epoch: 106; val_loss: 0.20420533418655396\n",
      "2024-04-18 10:26:20,620 : INFO : epoch: 106; train_loss: 0.08188341557979584\n",
      "2024-04-18 10:26:34,189 : INFO : epoch: 107; val_loss: 0.204136922955513\n",
      "2024-04-18 10:26:34,193 : INFO : epoch: 107; train_loss: 0.08186426758766174\n",
      "2024-04-18 10:26:47,763 : INFO : epoch: 108; val_loss: 0.20419836044311523\n",
      "2024-04-18 10:26:47,765 : INFO : epoch: 108; train_loss: 0.08188513666391373\n",
      "2024-04-18 10:27:01,184 : INFO : epoch: 109; val_loss: 0.20416100323200226\n",
      "2024-04-18 10:27:01,186 : INFO : epoch: 109; train_loss: 0.08185350894927979\n",
      "2024-04-18 10:27:14,440 : INFO : epoch: 110; val_loss: 0.20414021611213684\n",
      "2024-04-18 10:27:14,443 : INFO : epoch: 110; train_loss: 0.08179119974374771\n",
      "2024-04-18 10:27:27,741 : INFO : epoch: 111; val_loss: 0.20410089194774628\n",
      "2024-04-18 10:27:27,743 : INFO : epoch: 111; train_loss: 0.08177778124809265\n",
      "2024-04-18 10:27:40,548 : INFO : epoch: 112; val_loss: 0.20415982604026794\n",
      "2024-04-18 10:27:40,551 : INFO : epoch: 112; train_loss: 0.08177842199802399\n",
      "2024-04-18 10:27:54,954 : INFO : epoch: 113; val_loss: 0.2041793018579483\n",
      "2024-04-18 10:27:54,957 : INFO : epoch: 113; train_loss: 0.08176825940608978\n",
      "2024-04-18 10:28:08,682 : INFO : epoch: 114; val_loss: 0.20403113961219788\n",
      "2024-04-18 10:28:08,685 : INFO : epoch: 114; train_loss: 0.08176583051681519\n",
      "2024-04-18 10:28:23,862 : INFO : epoch: 115; val_loss: 0.2041190266609192\n",
      "2024-04-18 10:28:23,865 : INFO : epoch: 115; train_loss: 0.08175648003816605\n",
      "2024-04-18 10:28:40,076 : INFO : epoch: 116; val_loss: 0.20413585007190704\n",
      "2024-04-18 10:28:40,078 : INFO : epoch: 116; train_loss: 0.08177366852760315\n",
      "2024-04-18 10:28:56,566 : INFO : epoch: 117; val_loss: 0.20413745939731598\n",
      "2024-04-18 10:28:56,571 : INFO : epoch: 117; train_loss: 0.08175205439329147\n",
      "2024-04-18 10:29:13,528 : INFO : epoch: 118; val_loss: 0.20411330461502075\n",
      "2024-04-18 10:29:13,531 : INFO : epoch: 118; train_loss: 0.08175607025623322\n",
      "2024-04-18 10:29:31,864 : INFO : epoch: 119; val_loss: 0.20415233075618744\n",
      "2024-04-18 10:29:31,867 : INFO : epoch: 119; train_loss: 0.0817493200302124\n",
      "2024-04-18 10:29:49,273 : INFO : epoch: 120; val_loss: 0.20414870977401733\n",
      "2024-04-18 10:29:49,294 : INFO : epoch: 120; train_loss: 0.08175904303789139\n",
      "2024-04-18 10:30:05,139 : INFO : epoch: 121; val_loss: 0.20416660606861115\n",
      "2024-04-18 10:30:05,142 : INFO : epoch: 121; train_loss: 0.08174143731594086\n",
      "2024-04-18 10:30:20,201 : INFO : epoch: 122; val_loss: 0.2041378617286682\n",
      "2024-04-18 10:30:20,204 : INFO : epoch: 122; train_loss: 0.0817442536354065\n",
      "2024-04-18 10:30:34,300 : INFO : epoch: 123; val_loss: 0.20410306751728058\n",
      "2024-04-18 10:30:34,303 : INFO : epoch: 123; train_loss: 0.08173786103725433\n",
      "2024-04-18 10:30:48,225 : INFO : epoch: 124; val_loss: 0.20419761538505554\n",
      "2024-04-18 10:30:48,227 : INFO : epoch: 124; train_loss: 0.08174525201320648\n",
      "2024-04-18 10:31:01,640 : INFO : epoch: 125; val_loss: 0.20412981510162354\n",
      "2024-04-18 10:31:01,643 : INFO : epoch: 125; train_loss: 0.08173933625221252\n",
      "2024-04-18 10:31:15,555 : INFO : epoch: 126; val_loss: 0.20413590967655182\n",
      "2024-04-18 10:31:15,557 : INFO : epoch: 126; train_loss: 0.08172649890184402\n",
      "2024-04-18 10:31:29,900 : INFO : epoch: 127; val_loss: 0.2041800618171692\n",
      "2024-04-18 10:31:29,902 : INFO : epoch: 127; train_loss: 0.0817030668258667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey.golda/Library/Caches/pypoetry/virtualenvs/msu-interpol--lw2ADYE-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "data_module = InterpolDataModule(hyperparams=hyperparams_dict)\n",
    "model = InterpolRegressor(hyperparams=hyperparams_dict)\n",
    "# model = InterpolRegressor.load_from_checkpoint(f'./wandb_local_logs/MSU_interpol/blooming-plasma-40/checkpoints/exp_name=0val_loss=6.43574-epoch=14.ckpt', hyperparams=hyperparams_dict)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=hyperparams_dict.get('max_epochs'),\n",
    "                     accelerator='cpu',\n",
    "                     logger=wandb_logger,\n",
    "                     enable_progress_bar=False)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f33d989",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:31:42.244997Z",
     "start_time": "2024-04-18T09:31:36.178417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43a6c007058744d981e9085cbc3e6590"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr_scheduler_monitoring</td><td>██▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>▇▃▁▄▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▄▁▄▄▃▇▁▄▄▃▃▆▄▆▅▆▆▅▇▇▇▆▇██████▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_mae</td><td>▄▁▂▂▃▆▂▃▃▃▃▅▄▅▄▅▅▅▇▇▆▆▇██████▇▇██▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>127</td></tr><tr><td>lr_scheduler_monitoring</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.0817</td></tr><tr><td>train_mae</td><td>0.30395</td></tr><tr><td>trainer/global_step</td><td>10240</td></tr><tr><td>val_loss</td><td>0.20418</td></tr><tr><td>val_mae</td><td>0.29827</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">denim-jazz-12</strong> at: <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/sfp3r23n' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol_by_energy/runs/sfp3r23n</a><br/> View job at <a href='https://wandb.ai/msu_ai/msu_interpol_by_energy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2Mzc5MTAwNA==/version_details/v0' target=\"_blank\">https://wandb.ai/msu_ai/msu_interpol_by_energy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2Mzc5MTAwNA==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb_local_logs/wandb/run-20240418_100530-sfp3r23n/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
